{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford HPC Advisory Council Conference\n",
    "\n",
    "- February 07-08, 2017 \n",
    "- Munger Conference Center - Paul Brest Hall\n",
    "- Stanford University\n",
    "- Website: http://www.hpcadvisorycouncil.com/events/2017/stanford-workshop/\n",
    "\n",
    "## Agenda\n",
    "\n",
    "### Day One: Tuesday, 07 February, 2017\n",
    "\n",
    "- **9:00-9:10:** Welcome\n",
    "\t- Steve Jones & Gilad Shainer\n",
    "- **9:10-9:55:** Keynote: HPC Meets Big Data: Accelerating Hadoop, Spark, and Memcached\n",
    "\t- The Ohio State University\n",
    "\t- DK Panda\n",
    "- **9:55-10:15:** Industry Insights: Architecting Flash for Scale and Performance in HPC\n",
    "\t- Weka.IO\n",
    "\t- Liran Zvibel\n",
    "- **10:15-11:00:** Keynote: Singularity: Containers for Science, reproducibility, and HPC \n",
    "\t- Lawrence Berkeley National Laboratory  \n",
    "\t- Greg Kurtzer\n",
    "- **11:00-12:00:** Tutorial: 0 to 60 w/Intel® HPC Orchestrator ~ from Bare Metal to Production HPC\n",
    "\t- Intel Corporation\n",
    "\t- Stanford High Performance Computing Center\n",
    "\t- David Lombard & Steve Jones\n",
    "- **12:00-13:00:** Lunch\n",
    "- **13:00-13:30:** Industry Insights: A Fresh Look at High Performance Computing\n",
    "\t- Huawei Enterprise \n",
    "\t- Francis Lam\n",
    "- **13:30-14:00:** Best Practices: The Era of Self-Tuning Servers\n",
    "\t- DatArcs\n",
    "\t- Tomer Morad\n",
    "- **14:00-14:45:** Tutorial: Towards Exascale Computing with Fortran 2015\n",
    "\t- Sourcery Institute \n",
    "\t- National Center for Atmospheric Research \n",
    "\t- Damian Rouson & Alessandro Fanfarillo\n",
    "- **14:45-15:00:** Break\n",
    "- **15:00-15:30:** Best Practices: Large Scale Multiphysics\n",
    "\t- Cascade Technologies \n",
    "\t- Frank Ham\n",
    "- **15:30-16:00:** Industry Insights: Hot Technology Topics in 2017 \n",
    "\t- OrionX.net \n",
    "\t- Shahin Khan\n",
    "- **16:00-17:00:** Panel: The Exascale Endeavor \n",
    "\t- Moderator: Gilad Shainer\n",
    "\t- Panel: John Shalf, DK Panda, Frank Ham, Addison Snell\n",
    "- **17:00-17:15:** Recap and Day 2 Preview\n",
    "\t- Steve Jones & Gilad Shainer\n",
    "- **17:15-19:00:** Twilight Tutorial: Machine Learning Bootcamp\n",
    "\t- Mellanox Technologies\n",
    "\t- NVIDIA Deep Learning Institute\n",
    "\t- Scot Schultz & Julie Bernauer\n",
    "\n",
    "### Day Two: Wednesday, 08 February, 2017\n",
    "\n",
    "- **9:00:** Welcome\n",
    "\t- Gilad Shainer & Steve Jones\n",
    "- **9:05-9:50:** Visionary Perspectives: Where Computing is Going…\n",
    "\t- Lawrence Berkeley National Laboratory\n",
    "\t- John Shalf\n",
    "- **9:50-10:20:** Best Practices: Designing HPC & Deep Learning Middleware for Exascale Systems\n",
    "\t- The Ohio State University\n",
    "\t- DK Panda\n",
    "- **10:20-10:50:** Best Practices: Multi-Physics Methods, Modeling, Simulation & Analysis\n",
    "\t- Stanford University, Center for Turbulence Research\n",
    "\t- Mahdi Esmaily\n",
    "- **10:50-11:20:** Best Practices: State of Linux Containers\n",
    "\t- **Gaikai Inc.\n",
    "\t- Christian Kniep\n",
    "- **11:20-12:00:** Industry Insights: HPC Computing Trends\n",
    "\t- Intersect360 Research \n",
    "\t- Addison Snell\n",
    "- **12:00-13:00:** Lunch\n",
    "- **13:00-13:20:** Best Practices: Application Profiling at the HPCAC High Performance Center\n",
    "\t- HPC Advisory Council \n",
    "\t- Pak Lui\n",
    "- **13:20-13:50:** Best Practices: Containerizing Distributed Pipes \n",
    "\t- Gaikai Inc. \n",
    "\t- Hagen Toennies\n",
    "- **13:50-14:20:** Industry Insights: Deep Learning & HPC: New Challenges for Large Scale Computing\n",
    "\t- NVIDIA \n",
    "\t- Julie Bernauer\n",
    "- **14:20-15:00:** Tutorial: In-Network Computing SHARP Technology for MPI Offloads\n",
    "\t- Mellanox Technologies \n",
    "\t- Devendar Bureddy\n",
    "- **15:00-15:15:**  Break\n",
    "- **15:15-15:45:** HPC Impact: Using HPC in a Cohort Study of the Health Effects of Handgun Ownership in California\n",
    "\t- Stanford University School of Medicine & Stanford Law School  \n",
    "\t- Yifan Zhang & David M. Studdert\n",
    "- **15:45-16:45:** End Note: Computing of the Future\n",
    "\t- IBM Research Almaden \n",
    "\t- Jeffrey Welser\n",
    "- **16:45:** Raffle & Wrap Up\n",
    "\t- Steve Jones & Gilad Shainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keynote: HPC Meets Big Data: Accelerating Hadoop, Spark & Memcached\n",
    "\n",
    "### DK Panda, The Ohio State University\n",
    "\n",
    "- Introduction to Big Data Analytics and Trends\n",
    "    - Running High Performance Data Analysis (HPDA) workloads in the cloud is gaining popularity\n",
    "        - => 27% of cloud deployments\n",
    "- ** Diagram: Data Management and Processing on Modern Clusters**\n",
    "- Trends for Commodity Computing in the Top 500 List\n",
    "- Drivers for Modern HPC Cluster and Data Center Architecture\n",
    "    - RDMA: Remote Direct Memory Access\n",
    "- Trends in HPC Technologies\n",
    "    - Advancements\n",
    "        - InfiniBand\n",
    "        - Ethernet/iWARP\n",
    "        - RDMA over Converged Enhanced Ethernet (RoCE)\n",
    "    - Delivers excepllent Latency, Bndwidt, CPU util\n",
    "    - Redesigns of HPC middleware\n",
    "    - SSDs, GPUs\n",
    "- Interconnects and Protocols in OpenFabrics Stack for HPC (http://openfabrics.org)\n",
    "- Large-scale InfiniBand Installations\n",
    "    - 37% using IB Clusters in 11/2016 Top 500\n",
    "- Open Standard InfiniBand Networking Tech\n",
    "    - Multi Operations\n",
    "        - Send/Rcv\n",
    "        - RDMA R/W\n",
    "        - Atomic Operations (very unique)\n",
    "            - High perf & scalable impl's of distributed locks, semaphores, collective comm operations\n",
    "- Overview of the MVAPICH2 Project\n",
    "    - http://mvapich.cse.ohio-state.edu\n",
    "- HPC Clusters w/ High Performance Interconnect and Storage Architectures' Benefit for Big Data Apps?\n",
    "    - HPC Techs\n",
    "    - Bottlenecks\n",
    "    - RDMA-enabled hpi\n",
    "    - High performance storage\n",
    "- Designing Comm and I/O Libraries for Big Data Systems: Challenges\n",
    "- Can Big Data Processing Systems be Design w/ HPNetworks and Protocols?\n",
    "    - Their Approach: App -> Osu Design -> Verbs Interface -> 10/40/GigE\n",
    "- The High-Proforhace Big Data (HiBD) Project\n",
    "    - http://hibd.cse.ohio-state.edu\n",
    "- RDMA for Apache Hadoop 2.x Distribution\n",
    "- RDMA for Apache Spark Distribution\n",
    "- HiBD Packages on SDSC Comet and Chameleon Cloud\n",
    "    - Also available for Apache on Chameleon Cloud\n",
    "        - http://chameleoncloud.org/appliances/17\n",
    "- RDMA for Memcached Distribution\n",
    "- OSU HiBD Micro-Benchmark (OHB) Suite - HDFS, Memcached, HBase and Spark\n",
    "- Acceleration Case Studies and Perf Evaluation\n",
    "    - Basic Designs\n",
    "        - HDFS and MapReduce\n",
    "        - Spark\n",
    "        -Hadoop RPC and HBase\n",
    "        - Memcached\n",
    "        -HDFS with Memcached-based Burst Buffer\n",
    "    - Advanced\n",
    "        - HDFS and MapReduce with NVRAM\n",
    "        - Accelerating Big Data I/O (Lustre + Burst-Buffer)\n",
    "        - Efficient Indexing Techniques on HBase with RDMA\n",
    "        - Tuning and Profiling\n",
    "- Design Overview of HDFS with RDMA\n",
    "- Design Overview of MapReduce with RDMA\n",
    "- Design Overview of Spark with RDMA\n",
    "- Accelerating Hybrid Memcached with RDMA, Non-blocking Extensions and SSDs\n",
    "    - Hybrid RAM+SSD slab management for higher data retention\n",
    "    - Non-blocking API extensions\n",
    "- Accelerating I/O Perf of Big Data Analytics through HDFS with RDMA-Memcached based Birth Buffer\n",
    "- Advanced Designs!\n",
    "- NVM: Non-Volatile Memory\n",
    "- NVFS: NVM and RDMA-aware HDFS\n",
    "    - RDMA over NVM\n",
    "    - HDFS I/O with NVM\n",
    "        - Block Access\n",
    "        - Memory Access\n",
    "    - Hybrid Design => NVM + SSD\n",
    "- NVRAM-Assisted Map Spilling in HOMR\n",
    "- Birth-Buffer Over Lustre for Accelerating Big Data I/O (Boldio)\n",
    "- Accelerating Indexing Techniques on HBse with RDMA\n",
    "- Challenges of Tuning and Profiling\n",
    "- MR-Advisor\n",
    "- Tuning Experiments with MR-Advisor (TACC Stampede)\n",
    "- Virtualization-aware and Automatic Topology Detection Schemes in Hadoop on InfiniBand\n",
    "- [Network-Based Computing Laboratory](http://nowlab.cse.ohio-state.edu)\n",
    "- [The High-Performance Big Data Project](http://hibd.cse.ohio-state.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry Insights: Architecting Flash for Scale and Performance in HPC\n",
    "\n",
    "### Liran Zvibel, Weka.IO @lirazvibel\n",
    "\n",
    "- Leveraging the Shift to FLash\n",
    "    - SSDs changed everything\n",
    "        - Data Structures\n",
    "        - Power Failture\n",
    "        - Endurance\n",
    "        - Reliability\n",
    "        - Latency\n",
    "- Architecting for Scale\n",
    "    - Triple Replication Costs => 3x cost, 1/3 life\n",
    "- Summary\n",
    "    - SSD has transformational impact on I/O\n",
    "        - Data Structures, I/O path, resliency, endurance, cost\n",
    "    - Legacy file systems will not solve the small file challenge\n",
    "    - Requires a next gen file system\n",
    "        - Flash centric\n",
    "        - Virtualized, tiered, scalable, modern interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10:15-11:00 Keynote: Singularity: Containers for Science, reproducibility, and HPC\n",
    "\n",
    "### Lawrence Berkeley National Laboratory, Greg Kurtzer\n",
    "\n",
    "- **Containers:** encapsulations of system environments\n",
    "    - Created to solve: Micro-service Virtualization\n",
    "- Scientists are like pirates!\n",
    "- Docker => Most well-known container used\n",
    "    - The bad: HPC centers don't allow it\n",
    "- About Singularity\n",
    "    - Developed from necessity\n",
    "    - Released April 2016\n",
    "    - Created for scientists, users, HPC engineers, Linux developers\n",
    "- Design Goals\n",
    "    - Support \"Mobility of Compute\", ageility, BYOE, portability\n",
    "    - Single file based container images\n",
    "        - Distribution, archiving, sharing\n",
    "        - Efficient parallel file systems\n",
    "    - No system, architectural or workflow changes to integrate on HPC\n",
    "    - Limits user's privileges (inside user == outside user)\n",
    "    - No root owned container daemon\n",
    "    - Simple integration with resource managers, InfiniBand, GPUs, MPI, file systems, supports multiple architectures (x86_64, PPC, ARM, etc)\n",
    "- Access and Privilege\n",
    "    - Blocks user's ability to increase their own usage\n",
    "    - To be rooti in a container, you need to be root outside the container\n",
    "- Container Image types Supported\n",
    "    - Singularity 1.x\n",
    "    - SquashFS\n",
    "    - Tarballs (requires caching)\n",
    "    - Flat directories (chroots)\n",
    "- Containerize MPI Latency comparison over IB\n",
    "    - OpenMPI 2.0.1 with OSU Micro Benchmrks 5.3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: 0 to 60 w/Intel® HPC Orchestrator ~ from Bare Metal to Production HPC\n",
    "\n",
    "### David Lombard & Steve Jones, Intel Corporation, Stanford High Performance Computing Center\n",
    "\n",
    "- What is OpenHPC\n",
    "- Intel Parallel Studio XE\n",
    "- Management\n",
    "    - Control\n",
    "        - Cluster Checker\n",
    "        - genders\n",
    "        - mrsh\n",
    "        - pdsh\n",
    "        - powerman\n",
    "        - prun\n",
    "        - ssh\n",
    "    - Monitoring\n",
    "- Workload Management\n",
    "    - WLM: Workload managers - run user workflows on system\n",
    "        - Very important to user community -> modularity driver\n",
    "    - Two leading WLMs supported\n",
    "        - SLURM\n",
    "        - PBS Professional\n",
    "- Third party packages\n",
    "- Moving Forward\n",
    "    - http://openhpc.community\n",
    "    - mike.sheppard@intel.com\n",
    "    - thomas.a.krueger@intel.com4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13:00-13:30 Industry Insights: A Fresh Look at High Performance Computing\n",
    "\n",
    "### Francis Lam, Director, Product Management, Huawei Enterprise\n",
    "\n",
    "- HPC Solutions\n",
    "    - Petascale System: Direct Liquid Cooling\n",
    "    - Workload Optimization: Ecosystem Partnership\n",
    "    - Reduce Complexity: More Performance / $\n",
    "    - Design for Growth: HPC Private Cloud\n",
    "- Huawei Advantages\n",
    "    - Energy efficieny => Very important part of their design\n",
    "        - 100% in-house\n",
    "    - Accelerate Workload\n",
    "    - Adapt to Change\n",
    "- End-To-End Green HPC Design\n",
    "    - Right-sized Power\n",
    "    - HVDC\n",
    "    - Energy Efficient Server Design\n",
    "    - Air Containmnet \n",
    "    - Free Air Cooling\n",
    "    - Hihger Temp\n",
    "    - Hot Water\n",
    "    - HW Accel\n",
    "    - SSD Storage => 60% less power consump than HDD\n",
    "    - In-Memory Computing => 90% less power consump than HDD\n",
    "    - **Green IT Reduces Energy Bill & CO2 Emission, Extends DC Life, Lowers TCO**\n",
    "- Huawei Fusionserver Liquid Cooling System\n",
    "- Workload Characteristics\n",
    "    - Scalability\n",
    "    - I/O\n",
    "    - Storage\n",
    "    - Memory capacity\n",
    "    - Memory banwidth\n",
    "    - CPU\n",
    "- Bottom Line\n",
    "    - Select a form factor that's bigger\n",
    "    - Wide range of modules allows very high density\n",
    "- Accelerate High Memory Bandwidth Workload with Fat Nodes\n",
    "- Future\n",
    "    - Enabling HPC Cloud\n",
    "    - Software Defined\n",
    "    - Big Data Acceleration\n",
    "    - Deep Learning\n",
    "- HPC Clouid Case Study\n",
    "    - Key Reqs\n",
    "        - ECS -> EBS -> VPC / EIP / Direct Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13:30-14:00 Best Practices: The Era of Self-Tuning Servers\n",
    "\n",
    "### Tomer Morad, DatArcs\n",
    "\n",
    "- Introduction to Tuning\n",
    "    - Knobs: settings on a server that...\n",
    "        - Can be changed in real time\n",
    "        - Affect perf/energy efficiency\n",
    "        - Retains correctness\n",
    "    - Tuning\n",
    "        - Proc of finding the best setting of a knobs\n",
    "        - Tuning example\n",
    "- Demo: Prefetching\n",
    "    - Showed 2 applications w/ and w/o prefetching\n",
    "    - One app prefers prefetch over the other\n",
    "- Program Phases\n",
    "    - Phase 1: Pro-prefetching\n",
    "    - Phase 1: Anti-prefetching\n",
    "- Limitations of manual tuning\n",
    "    - Too many to do manually\n",
    "    - Dependencies on different knobs\n",
    "    - Knob settings depend on HW\n",
    "    - & Apps and input data\n",
    "    - No practical way to \"see\" program phases\n",
    "- The era of self-tuning servers => better for servers to tune themselves\n",
    "- Phases: Baseline phase -> Learning Phase -> Optimization Phase -> Static Tuning Phase\n",
    "- High-Speed Networking Results\n",
    "- Trends\n",
    "    - Number of knobs increases => HW becomes more complex = more knobs\n",
    "        - Lots of heterogeneity in the data set => leads to more nulls\n",
    "    - Expertise is becoming scarce => less people who understand it\n",
    "        - Wrong to assume cloud does tuning automatically\n",
    "    - HW matters\n",
    "        - More tuning opportunities\n",
    "        - VMs vs. bare metal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **14:00-14:45:** Tutorial: Towards Exascale Computing with Fortran 2015\n",
    "\n",
    "### Damian Rouson & Alessandro Fanfarillo, Sourcery Institute, National Center for Atmospheric Research \n",
    "\n",
    "- Outline\n",
    "    - Parallelism in Fortran 2008\n",
    "    - Exascale challenges\n",
    "- PGAS\n",
    "    - Array corrolaries\n",
    "    - Scalar corrolaries\n",
    "    - Square brackets there just for communication\n",
    "- Exascale Challenges -> Fortran 2015 Responses \n",
    "    - Higher parallelism -> events, collectives, atomics\n",
    "    - Heterogenous HW -> events\n",
    "    - Higher failure rates -> failed-image detection\n",
    "    - Locality Control -> Teams\n",
    "- Teams: Groupings of images that can readily execute independently of other images\n",
    "- Compiler Support & Video Tutorials\n",
    "    - Download free VM at http://sourceryinstitute.org/store\n",
    "    - Video tutorials at http://sourceryinstitute.org/videos\n",
    "- Related Research Literature\n",
    "    - Coarray Fortran (CAF)\n",
    "        - http://opencoarrays.org/publications\n",
    "        - http://sourceryinstitute.org/publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14:45-15:00: Break\n",
    "\n",
    "## 15:00-15:30: Best Practices: Large Scale Multiphysics\n",
    "\n",
    "### Frank Ham, Cascade Technologies\n",
    "\n",
    "- Super collaboration\n",
    "- Timeline\n",
    "    - 2014: Full speed full load test -> instability\n",
    "    - 2015-2016\n",
    "        - Joint Dev Agreement b/w GE and Cascade for Gas Turbine\n",
    "- HPC Partnerships:\n",
    "    - GE\n",
    "    - Oak Ridge National Laboratory\n",
    "    - Cascade Technologies\n",
    "- NASA: CFD Vision 2030 Study: A Path to Revolutionary Computational Aerosciences\n",
    "- Starting point: Cascade's CharLES Solver 2015\n",
    "    - Scaled to 1 million+ cores\n",
    "    - Finite Volume Method\n",
    "    - Explicity RK3 time advancement\n",
    "- How to do grid generation on HPC resource?\n",
    "    - Grid generation a bottleneck\n",
    "    - Grids not going away\n",
    "    - Part of the simulation process\n",
    "- Clipped Voronoi Diagrams\n",
    "    - Key Tech: Robust direct generation of the clipped voronoi diagrams inside an arbitrary hull\n",
    "    - Basically, Euclidean Distance\n",
    "    - Voronoi grid justified by its points\n",
    "- Voronoi Generating Points\n",
    "    - HCP: Hexagonal close-packed lattice\n",
    "    - Similar to pixel rendering\n",
    "    - Parallel, scalable\n",
    "- Boundary Recovery using Lloyd Iteration\n",
    "- CPU-side solver optimizations\n",
    "    - Restructuring data for cached efficiency\n",
    "    - Aggressive overlapping computation/communication (latency hiding)\n",
    "    - Vectorizing instructions, aggressive inlinng\n",
    "    - Improved partitioning algorithm (native, no longer ParMetis)\n",
    "    - LLVM bytrecode: load operationsfor internal flux calculation (12/2015)\n",
    "    - Reordering instructions, algebraic factorizations, storing immediate vars in internal flux routines to reduce repeated load instructions\n",
    "- Simulations are running fast -> How do learn from them?\n",
    "- Solution: Images + metadata\n",
    "    - Instrument solvers to directly write images + metadata\n",
    "    - Images become Cartesian arrays of probes\n",
    "    - Dimensional reduction + lossless compression at soruce\n",
    "    - Intrinsic value\n",
    "- Quantitative data analysis from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15:30-16:00: Industry Insights: Hot Technology Topics in 2017 \n",
    "\n",
    "### Shahin Khan, OrionX.net \n",
    "\n",
    "- Information Age\n",
    "    - Extraction -> Digitization\n",
    "    - Process -> IT Systems\n",
    "    - Interpretation -> HPC\n",
    "        - It's a big deal\n",
    "        - The only way to make sense of the info age and the challenges it's posing for us\n",
    "- Systems\n",
    "    - Even though the popularity of a system dies, it's not going away\n",
    "        - It's just all the action is in a new system\n",
    "- HPC\n",
    "    - If your spreadsheet has 100 rows, it's a spreadsheet\n",
    "    - If your spreadsheet has 1 billion rows, it's a HPC\n",
    "    - Digitization -> Data -> STEM IT -> HPC\n",
    "    - Knowing science, knowing data is what you need to understand data applications\n",
    "- Hot Topics\n",
    "- IoT\n",
    "- BitCoin and AltCoin\n",
    "    - How to avoid cheaters? => Make cheating computationally expensive\n",
    "- AI = HPAI\n",
    "    - Essence of AI: Intelligence is Computable\n",
    "- Apps: Containers, Automation, Fabric\n",
    "    - Overtime: logic became more distributed -> now called microservices\n",
    "- Data Center Predictions\n",
    "- Dealing with Change\n",
    "    - Change is best when it is your own idea. Change is worst when it is imposed on you.\n",
    "    - When change happens that is undesirable or unexpected, you have to reinvent.\n",
    "- Q&A\n",
    "    - Our view of autonomy is a fantasy. Autonomy can only happen in a highly rigorous scope.\n",
    "    - What will make a difference for AR/VR: 5G Network\n",
    "        - 10-12 MB/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16:00-17:00: Panel - The Exascale Endeavor \n",
    "\n",
    "- Moderator: Gilad Shainer\n",
    "- Panel: John Shalf, DK Panda, Frank Ham, Addison Snell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17:00-17:15: Recap and Day 2 Preview\n",
    "\n",
    "- Steve Jones & Gilad Shainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17:15-19:00:  Twilight Tutorial: Machine Learning Bootcamp\n",
    "\n",
    "### Scot Schultz & Julie Bernauer, Mellanox Technologies, NVIDIA Deep Learning Institute\n",
    " \n",
    "- Training -> Inferencing\n",
    "     - Scalable Performance -> Throughput + Efficiency -> Data/Users ->\n",
    "- Scalability\n",
    "     - Strong Scaling: 2x machines => solve in 1/2 time\n",
    "     - Weak Scaling: Data is 2x big => 2x machines solves at constant rate\n",
    "- Four pillars\n",
    "    - RDMA\n",
    "    - SHARP: Scalable Hierarchical Aggregation Reduction Protocol\n",
    "    - NVMe over Fabrics\n",
    "    - GPUDirect-RDMA ASYNC\n",
    "\n",
    "#### Deep Learning and GPUs Intro and Hands-On Tutorial\n",
    "\n",
    "- ML, Neural Nets and Deep Learning\n",
    "    - ML\n",
    "    - NN\n",
    "    - DL\n",
    "- Object Recognition\n",
    "    - Traditional ML / Computer Vision Approach\n",
    "        - Raw Data -> Feature Extraction -> (Linear) Classifier -> Result\n",
    "    - The Deep Learning Approach\n",
    "        - Labelled Training Data -> Deep NN \"Model\" -> (<- Training Data) Object Class Predition\n",
    "- ANN: Collection of simple, trainable mathematical units that collective learn complex functions\n",
    "    - Input Layer -> Hidden Layer(s) -> Output Layer\n",
    "- Artificial Neuron: Methmatical Unit\n",
    "    - Biological Neuron\n",
    "    - Artificial Neuron\n",
    "- Deep Learning Approach\n",
    "    - Train: Test data -> DNN -> Errors\n",
    "    - Deploy: Test Data -> DNN -> Results\n",
    "- DNN\n",
    "    - Raw Data -> Low-level features -> Mid-level features -> High-level features\n",
    "    - Application components\n",
    "        - Task objective\n",
    "        - Training data\n",
    "        - Network architecture\n",
    "        - Learning algorithm\n",
    "- THe Big Bang in ML\n",
    "    - DNN, Big Data, GPU\n",
    "- DL Benefits\n",
    "    - Robust: Data automatically learned\n",
    "    - Generalizable: Same NN can be used for many apps and data types\n",
    "    - Scalable: Peformance improves with more data, method is massively parallelizable\n",
    "- GPUs for DL\n",
    "    - Deliver\n",
    "        - Prediction accuracy\n",
    "        - Faster results\n",
    "        - Smaller footprint\n",
    "        - Lower power\n",
    "- Compute Platform\n",
    "    - Google: 1k GPUs, 16000 cores, $5 million\n",
    "    - Stanford: 12 GPUs, 18000 cores, $20000\n",
    "- NVIDIA Deep Learning SKD: High Perf GPU-Accel for DL\n",
    "    - Apps\n",
    "    - FW\n",
    "    - DL FW\n",
    "- Obj classification -> Obj detection\n",
    "- Segmentation => Useful for self-driving cars.. tags everything it sees\n",
    "- Captioning: Convolutional NN + Recurrent NN\n",
    "- Text Generation: RNNs\n",
    "- Generation: GANs\n",
    "- Four Labs: http://nvlabs.qwiklab.com\n",
    "    1. Getting STarted with DL (DIGITS, simple classification)\n",
    "    2. Intro to DL (Python, Caffe, DIGITS, classification)\n",
    "    3. Approaches to Obj Detection using DIGITS (python, DIGITS, detection)\n",
    "    4. NVIDIA-Docker (cuda, MNIST, Tensorflow, in a container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford HPC Advisory Council Conference 2017 - Day 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **9:00:** Welcome\n",
    "\t- Gilad Shainer & Steve Jones\n",
    "- **9:05-9:50:** Visionary Perspectives: Where Computing is Going…\n",
    "\t- Lawrence Berkeley National Laboratory\n",
    "\t- John Shalf\n",
    "- **9:50-10:20:** Best Practices: Designing HPC & Deep Learning Middleware for Exascale Systems\n",
    "\t- The Ohio State University\n",
    "\t- DK Panda\n",
    "- **10:20-10:50:** Best Practices: Multi-Physics Methods, Modeling, Simulation & Analysis\n",
    "\t- Stanford University, Center for Turbulence Research\n",
    "\t- Mahdi Esmaily\n",
    "- **10:50-11:20:** Best Practices: State of Linux Containers\n",
    "\t- **Gaikai Inc.\n",
    "\t- Christian Kniep\n",
    "- **11:20-12:00:** Industry Insights: HPC Computing Trends\n",
    "\t- Intersect360 Research \n",
    "\t- Addison Snell\n",
    "- **12:00-13:00:** Lunch\n",
    "- **13:00-13:20:** Best Practices: Application Profiling at the HPCAC High Performance Center\n",
    "\t- HPC Advisory Council \n",
    "\t- Pak Lui\n",
    "- **13:20-13:50:** Best Practices: Containerizing Distributed Pipes \n",
    "\t- Gaikai Inc. \n",
    "\t- Hagen Toennies\n",
    "- **13:50-14:20:** Industry Insights: Deep Learning & HPC: New Challenges for Large Scale Computing\n",
    "\t- NVIDIA \n",
    "\t- Julie Bernauer\n",
    "- **14:20-15:00:** Tutorial: In-Network Computing SHARP Technology for MPI Offloads\n",
    "\t- Mellanox Technologies \n",
    "\t- Devendar Bureddy\n",
    "- **15:00-15:15:**  Break\n",
    "- **15:15-15:45:** HPC Impact: Using HPC in a Cohort Study of the Health Effects of Handgun Ownership in California\n",
    "\t- Stanford University School of Medicine & Stanford Law School  \n",
    "\t- Yifan Zhang & David M. Studdert\n",
    "- **15:45-16:45:** End Note: Computing of the Future\n",
    "\t- IBM Research Almaden \n",
    "\t- Jeffrey Welser\n",
    "- **16:45:** Raffle & Wrap Up\n",
    "\t- Steve Jones & Gilad Shainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
